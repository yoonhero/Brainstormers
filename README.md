# 2023 R&E 프로젝트 선행 연구 공부 일지

## Prerequisits

-   Brain Activity to Image

    -   [x] Hyperrealistic neural decoding for reconstructing faces from fMRI activations via the GAN latent space
    -   [ ] Brain2Image: Converting Brain Signals into Images
    -   [ ] End-to-End Deep Image Reconstruction From Human Brain Activity
    -   [ ] ThoughtViz: Visualizing Human Thoughts Using Generative Adversarial Network
    -   [ ] Towards a Passive BCI to Induce Lucid Dream

-   Brain Signal Analysis

    -   [ ] 합성곱 신경망을 이용한 뇌파 해석 기법
    -   [ ] 연세대학교 뇌 관련 PPT
    -   [ ] A Novel Bi-hemispheric Discrepancy Model for EEG Emotion Recognition
    -   [ ] EEG emotion recognition using dynamical graph convolutional neural networks
    -   [ ] Investigating critical frequency bands and channels for EEG-based emotion recognition with deep neural networks

-   Vision

    -   Classification
        -   [x] AlexNet
        -   [x] VGGNet
        -   [x] GoogLeNet
        -   [x] Deep Residual Learning for Image Recognition (ResNet) (2015)
        -   [ ] Rethinking the Inception Architecture for Computer Vision (Inception-v2~3) (2016)
        -   [ ] MobileNet
        -   [ ] EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks(EfficientNet) (2019)
        -   [ ] Vision Transformer
    -   Object Detection

        -   [ ] R-CNN
        -   [ ] Fast R-CNN
        -   [ ] Mask R-CNN
        -   [ ] YOLO

    -   Segmentation

        -   [ ] U-Net: Convolutional Networks for Biomedical Image Segmentation

    -   [ ] Active Learning for CNN : A Core-set Approach (2018)
    -   [ ] Learning Loss for Active Learning (2019)
    -   [ ] Learning Deep Architectures for AI

-   Generative Model

    -   Text

        -   [ ] Attention Is All You Need (Transformer)
        -   [ ] Improving Language Understanding by Generative Pre-Training (GPT1)
        -   [ ] Language Models are Unsupervised Multitask Learners (GPT2)
        -   [ ] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (BERT)

    -   Caption

        -   [ ] All You May Need for VQA are Image Captions
        -   [ ] CLIP: Learning Transferable Visual Models From Natural Language

    -   Generative Models

        -   [ ] Pixel Recurrent Neural Networks (PixelRNN)
        -   [x] Auto-Encoding Variational Bayes (VAE)
        -   [ ] Disentangling Variational Autoencoders

        -   GAN

            -   [x] Generative Adversarial Nets (GAN)
            -   [x] Conditional GAN
            -   [x] Pix2Pix
            -   [x] Cycle GAN
            -   [ ] StarGAN
            -   [x] Pixel2Style2Pixel
            -   [x] DCGAN
            -   [x] PGGAN
            -   [x] WGAN-GP
            -   [x] StyleGAN
            -   [ ] StyleGANv2
            -   [ ] SinGAN
            -   [ ] StyleCLIP
            -   [ ] Resolution Dependent GAN Interpolation for Controllable Image Synthesis Between Domains
            -   [ ] Analyzing and Improving the Image Quality of StyleGAN
            -   [ ] StyleGAN2 Distillation for Feed-forward Image Manipulation
            -   [ ] StyleCariGAN: Caricature Generation via StyleGAN Feature Map Modulation
            -   [ ] U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation

        -   [ ] Zero-Shot Text-to-Image Generation (DallE-1)
        -   [ ] Hierarchical Text-Conditional Image Generation with CLIP Latents (DallE-2)

        -   Diffusion Model
            -   Latent Diffusion
            -   [ ] Denoising Diffusion Probabilistic Model (Diffusion Model)
            -   [ ] Diffusion Models Beat GANs on Image Synthesis (Advanced Diffusion Model)
            -   Stable Diffusion
            -   [ ] Imagic: Text-Based Real Image Editing with Diffusion Models

-   To Study Again
    -   [x] CNN
    -   [x] RNN
    -   [x] LSTM
    -   [x] Seq2Seq
    -   [x] Embedding
    -   [x] Regularization
    -   [x] Optimization
